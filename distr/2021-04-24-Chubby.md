---
title: Google Chubby Lock Service
updated: 2021-04-24
---

Primary goal: Reliability and availability to a moderately large set of clients.
Throughput and storage capacity are secondary.
Client interface: whole-file reads and writes
advisory locks and notification of events such as file modification

Chubby lock for electing leader, used in GFS and Bigtable, to elect leader and store a small amount of metadata. Many services at Google that elect a primary or that partition data between their components need a mechanism for advertising the results, via read and write small files.

Consistent client cashing, rather than time-based cashing, so that developers do not have to choose a cache timeout.

Chubby serves small files to permit elected primaries to advertise themselves and their parameters.

An event notification mechanism to avoid polling.

To elect a master, which writes to file server: acquire a lock to become master, pass an additional integer, the lock acquisition count, with the write RPC, and add an if-statement to the file server to reject the write if the acquisition count is lower than the current value (to guard against delayed packets).

Chubby is intended to provide only coarse-grained locking, in which a primary holds a lock for hours or days (and not < sec).

Coarse-grained locks are acquired only rarely, so temporary lock server unavailability delays clients less.
The transfer of a lock from a client to a client may require costly recovery procedure, a fail-over of a lock server should not cause locks to be lost.

Chubby consists of a server and a library that client applications link against.
All communications between the servers and Chubby clients is mediated by the client library.

A Chubby cell consists of replicas, typically five.
The replicas elect a master using a distributed consensus protocol.
Replicas will not elect a different master for an interval of a few seconds (a.k.a, master lease).
Master lease is periodically renewed.

Replicas maintain copies of a simple database.
Only the master initiates reads and writes of the database.
All other replicas copy update from the master, sent using the consensus protocol.

Clients find the master by sending master location requests to the replicas listed in the DNS. Non-master replicas respond to such requests by returning the identity of the master. Once the master is located, clients direct all requests to it, either until the master ceases to respond or it indicates that it is no longer the master. Write requests are propagated via the consensus to all replicas. Write requests are acknowledged when the consensus has been reached.
Read requests are satisfied by the master alone.
This is safe provided the master lease has not expired, as no other master can possibly exist.

If a master fails, the other replicas run the election protocol when their master leases expire. A new master is typicall elected in a few seconds, but it could be as long as 30 seconds.

Chubby exports a file system interface, similar to but simpler than that of UNIX.

Each node, i.e., a file or a directory, has various metadata, including names of access control lists, where ACLs are files located in an ACL directory, and 4 monotonically increasing 64-bit numbers that allow clients to detect changes easily:
- instance number, greater than the instance number of any previous node with the same name
- content generation number (files only), which increases when the file's contents are written
- lock generation number, which increases when the node's lock transitions from free to held
- ACL generation number, which increases when the node's ACL names are written

Chubby also exposes a 64-bit file-content checksum, so clients may tell whether files differ.

Analogous to UNIX file descripters, clients open nodes to obtain handles, which include

- check digits that prevent clients from creating or guessing handles.
Hence full access control checks need be performed only when handles are created.

- a sequence number that allows a master to tell if a handle was created by it or by a previous master

- mode information provided at open time to allow the master to recreate its state if an old handle is presented to a newly restarted master

# Locks and sequencers

Each Chubby file and directory can act as a reader-writer lock: exclusive mode vs shared mode

Locks are advisory: holding a lock called F neither is necessary to access the file F nor prevents others from doing so.

At any time, a lock holder may request a sequencer, an opacque byte-string that describes the state of the lock immediately after acquisition. It contains the name of the lock, the mode in which it was acquired (shared vs exclusive), the lock generation number.

If a client releases a lock in the normal way, it is immediately available for other clients to claim. If a lock becomes free because the holder has failed or become inaccessible, the lock server will prevent other clients from claiming the lock for a period called the (configurable) lock-delay.

# Events

Chubby clients may subscribe to a range of events when they create a handle. These events are delivered to the client asynchronously. They include, file contents modified, Chubby master fail over, etc.

# API

Clients see a Chubby handle as a pointer to an opaque structure that supports various operations such as writing to and reading from the file, acquiring locks, getting sequencers, etc.

# Caching

To reduce read traffic, Chubby clients cache file data and node metadata in a consistent, write-through cache held in memory.
Cache is kept consistent by invalidations sent by the master.
The master keeps a list of what each client may be caching.

When file data or metadata is to be changed, the modification is blocked while the master sends invalidations for the data to every clients that may have cached it.
This mechanism sits on top of KeepAlive RPCs.
On receipt of an invaliation, a client flushes the invalidated state and acknowledges the master.
The modification proceeds only after the master knows that each client has invalidated its cache, either because the client acknowledged or the client allowed its cache lease to expire.


# Sessions and KeepAlives

A Chubby session between a Chubby cell and a Chubby client is maintained by periodic handshakes called KeepAlives.

Each session has an associated lease. The master extends the lease, when it responds to a KeepAlive RPC from the client.
On receiving a KeepAlive, the master blocks the RPC (does not allow it to return) until the client's previous lease interval is close to expring. The client initiates a new KeepAlive immediately after receiving the previous reply. Thus, the client ensures that there is almost always a KeepAlive call blocked at the master.
The KeepAlive reply is used to transmit events and cache invalidations back to the client.

# Fail-overs

When a master fails or loses its mastership, it discards its in-memory state about sessions, handles and locks. The authoritative timer for session leases runs at master, so until a new master is elected, the session lease timer is stopped, which is equivalent to extending the client's lease. If a new master is elected quickly, clients can contact the new master before their local approximate lease timers expire. If the election takes longer, clients flush their caches and wait for an interval, called the grace period, while trying to find a new master. The grace period allows sessions to be maintained across fail-overs that execeed the normal lease timeout.

During the grace period, it does not tear down its session, but it blocks all application calls on its API to prevent the application from observing inconsistent data.


# Backup

Every few hours, the master of a Chubby cell writes a snapshot of its database to a GFS server.

# Scaling

One Chubby master handles up to 100,000 clients.

The master may increase lease time from the default 12s up to around 60s when it is under heavy load, so it need process fewer KeepAlive RPCs.

Chubby clients cache data to reduce the number of calls they make on the master.

# Proxies

If a proxy handles N clients, KeepAlive traffic is reduced by a factor of N. => Why a factor of N??

# Use and behaviour

The key to scaling Chubby is not server performance. Reducing communication to the server can have far greater impact.

Chubby provides name service for most of Google's systems, because of its ability to provide swift name updates (in comparison to DNS) without polling each name individually. (Compare Chubby's explicit cache invalidation, which requires a constant rate of KeepAlive to maintain an arbitrary number of cache entries indefinitely at a client in the absence of changes, to DNS where entries are discarded when they have not been refreshed within the Time-To-Live period.


Chubby avoids recording sessions in the database at all, which helps avoid overloading the master. Instead, the master recreate sessions in the same way it recreates handles in the event of fail-overs. (As a result, a new master must now wait a full worst-case lease timeout before allowing operations to proceed, since it cannot know whether all sessions have checked-in.)

Once sessions can be recreated without on-disc state, proxy servers can manage sessions that the master is not aware of.


Next 4.5


---

Distributed lock service + simple fault-tolerant file system

Interfaces: file access, event notification, file locking

Used for
- coarse-grained long-term locks (hours or days, not < sec)
- store small amount of data associated with a name such as system configuration
- elect masters

Design priority: availability rather than performance

Chubby has at most one master, elected by a consensus protocol (paxos).

Requests from the client go to the master

Master gets a lease time, and re-run master selection after lease time expires of if the master fails.

When a Chubby node receives a proposal for a new master, it only accepts it if the old master's lease has expired

The client access Chubby via an API. Look up Chubby nodes via DNS. Ask any Chubby node for the master. File system interface.

Each file has name, data, access control list, and lock. (No modification/access time, nor partial reads/writes/ nor symbolic links nor moves)

Every file and directory can act as a reader-writer lock.
If a client fails, the lock will be unavailable for a lock-delay period (typically 1 minute.)

Chubby locks for leader election and using it to write to a file server.
- Participant who gets a lock together with a lock sequence count is the master.
- In each RPC to the server, send the sequence count.
- During request processing, the server reject packegs with old sequence count.

Chubby client cashing & master replication

At the client
- Data cached in memory by chubby clients. Cache is maintained by a Chubby lease, which can be invalidated
- All clients write through to the Chubby master

At the master
- Writes are propagated via Paxos to all Chubby replicas
  - Data updated in total order. Replicas remain synchronized
  - The master replies to clients after the consensus is reached
- Cache invalidations
  - Master keeps the list of what each client may be caching
  - Invalidations sent by the master and are acknowledged by clients
  - File is then cacheable again
- Chubby database is backed up to GFS every few hours

#### References

[The Chubby lock service for loosely-coupled distributed systems](https://dl.acm.org/doi/10.5555/1298455.1298487)

https://www.cs.rutgers.edu/~pxk/417/notes/pdf/12-dfs-slides.pdf
(accessed on 6.04.2021)
